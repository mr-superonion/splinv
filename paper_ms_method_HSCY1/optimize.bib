%Title: Coordinate descent algorithms
@ARTICLE{coordinateDescent-Wright2015,
	abstract = {
Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.
},
	affiliation = {Department of Computer Sciences, University of Wisconsin-Madison},
	author = {Wright, Stephen J.},
	copyright = {Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society},
	doi = {10.1007/s10107-015-0892-3},
	journal = {Mathematical Programming},
	keywords = {Coordinate descent; Randomized algorithms; Parallel numerical computing},
	language = {English},
	note = {The author was supported by NSF Awards DMS-1216318 and IIS-1447449, ONR Award N00014-13-1-0129, AFOSR Award FA9550-13-1-0138, and Subcontract 3F-30222 from Argonne National Laboratory.},
	number = {1},
	pages = {3-34},
	title = {Coordinate descent algorithms},
	volume = {151},
	year = {2015},
}

%Title: Pathwise coordinate optimization
@ARTICLE{pathwise-Friedman2007,
    abstract = {{We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class
of convex optimization problems. An algorithm of this kind has been proposed
for the \$L\_1\$-penalized regression (lasso) in the literature, but it seems to
have been largely ignored. Indeed, it seems that coordinate-wise algorithms are
not often used in convex optimization. We show that this algorithm is very
competitive with the well-known LARS (or homotopy) procedure in large lasso
problems, and that it can be applied to related methods such as the garotte and
elastic net. It turns out that coordinate-wise descent does not work in the
``fused lasso,'' however, so we derive a generalized algorithm that yields the
solution in much less time that a standard convex optimizer. Finally, we
generalize the procedure to the two-dimensional fused lasso, and demonstrate
its performance on some image smoothing problems.}},
    added-at = {2018-12-07T09:10:16.000+0100},
    archiveprefix = {arXiv},
    author = {Friedman, Jerome and Hastie, Trevor and H\"{o}fling, Holger and Tibshirani, Robert},
    biburl = {https://www.bibsonomy.org/bibtex/20d0a9118f986b6e1cbebcc266699ae80/jpvaldes},
    citeulike-article-id = {3979777},
    citeulike-linkout-0 = {http://arxiv.org/abs/0708.1485},
    citeulike-linkout-1 = {http://arxiv.org/pdf/0708.1485},
    citeulike-linkout-2 = {http://dx.doi.org/10.1214/07-aoas131},
    day = 14,
    doi = {10.1214/07-aoas131},
    eprint = {0708.1485},
    interhash = {846b5d7bbe708c8358773f089da30d19},
    intrahash = {0d0a9118f986b6e1cbebcc266699ae80},
    issn = {1932-6157},
    journal = {The Annals of Applied Statistics},
    keywords = {coordinate-descent, fused-lasso optimization total_variation},
    month = dec,
    number = 2,
    pages = {302--332},
    posted-at = {2017-11-20 14:09:13},
    priority = {2},
    timestamp = {2018-12-07T09:40:56.000+0100},
    title = {{Pathwise coordinate optimization}},
    url = {http://dx.doi.org/10.1214/07-aoas131},
    volume = 1,
    year = 2007
    }
